<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CAgbalumo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"aquietzero.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"width":null},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="No matter how small and unimportant a decision may seem, every decision you make is a trade-off between information gathering and information exploitation. The key intuition is that: exploration build">
<meta property="og:type" content="article">
<meta property="og:title" content="Balancing the gathering and use of information">
<meta property="og:url" content="https://aquietzero.github.io/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/index.html">
<meta property="og:site_name" content="NullSpace">
<meta property="og:description" content="No matter how small and unimportant a decision may seem, every decision you make is a trade-off between information gathering and information exploitation. The key intuition is that: exploration build">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-09-12T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-19T07:11:29.528Z">
<meta property="article:author" content="bifnudo">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="读书笔记">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://aquietzero.github.io/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://aquietzero.github.io/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/","path":"2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/","title":"Balancing the gathering and use of information"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Balancing the gathering and use of information | NullSpace</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BCZ3TL69CD"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-BCZ3TL69CD","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">NullSpace</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Stay hungry, stay foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">归档</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#the-challenge-of-interpreting-evaluative-feedback"><span class="nav-number">1.</span> <span class="nav-text">The challenge
of interpreting evaluative feedback</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#regret-the-cost-of-exploration"><span class="nav-number">1.1.</span> <span class="nav-text">Regret: The cost of
exploration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#approaches-to-solving-mab-environments"><span class="nav-number">1.2.</span> <span class="nav-text">Approaches to solving
MAB environments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#greedy-always-exploit"><span class="nav-number">1.2.1.</span> <span class="nav-text">Greedy: Always exploit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#random-always-explore"><span class="nav-number">1.2.2.</span> <span class="nav-text">Random: Always explore</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#epsilon-greedy-almost-always-greedy-and-sometimes-random"><span class="nav-number">1.2.3.</span> <span class="nav-text">Epsilon-greedy:
Almost always greedy and sometimes random</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decaying-epsilon-greedy-first-maximize-exploration-then-exploitation"><span class="nav-number">1.2.4.</span> <span class="nav-text">Decaying
epsilon-greedy: First maximize exploration, then exploitation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimistic-initialization-start-off-believing-its-a-wonderful-world"><span class="nav-number">1.2.5.</span> <span class="nav-text">Optimistic
initialization: Start off believing it’s a wonderful world</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#strategic-exploration"><span class="nav-number">2.</span> <span class="nav-text">Strategic exploration</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax-select-actions-randomly-in-proportion-to-their-estimates"><span class="nav-number">2.1.</span> <span class="nav-text">Softmax:
Select actions randomly in proportion to their estimates</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ucb-its-not-about-optimism-its-about-realistic-optimism"><span class="nav-number">2.2.</span> <span class="nav-text">UCB:
It’s not about optimism, it’s about realistic optimism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#thompson-sampling-balancing-reward-and-risk"><span class="nav-number">2.3.</span> <span class="nav-text">Thompson sampling:
Balancing reward and risk</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bifnudo"
      src="/assets/images/me.webp">
  <p class="site-author-name" itemprop="name">bifnudo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">123</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/aquietzero" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;aquietzero" rel="noopener me" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.douban.com/people/aquietzero" title="豆瓣 → https:&#x2F;&#x2F;www.douban.com&#x2F;people&#x2F;aquietzero" rel="noopener me" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyunhaosss@gmail.com" title="E-Mail → mailto:zhaoyunhaosss@gmail.com" rel="noopener me" target="_blank">E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://aquietzero.github.io/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/images/me.webp">
      <meta itemprop="name" content="bifnudo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NullSpace">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Balancing the gathering and use of information | NullSpace">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Balancing the gathering and use of information
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-13 00:00:00" itemprop="dateCreated datePublished" datetime="2023-09-13T00:00:00+08:00">2023-09-13</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>No matter how small and unimportant a decision may seem, every
decision you make is a trade-off between information gathering and
information exploitation.</p>
<p>The key intuition is that: exploration builds the knowledge that
allows for effective exploitation, and maximum exploitation is the
ultimate goal of any decision maker.</p>
<h1 id="the-challenge-of-interpreting-evaluative-feedback">The challenge
of interpreting evaluative feedback</h1>
<p>In MAB, the Q-function of action <span
class="math inline">\(a\)</span> is</p>
<p><span class="math display">\[
q(a) = \Bbb{E}[R_t | A_t = a]
\]</span></p>
<p>The best we can do in a MAB is represented by the optimal V-function,
or selecting the action that maximizes the Q-function.</p>
<p><span class="math display">\[
v_* = q(a_*) = \max_{a \in A} q(a)
\]</span></p>
<p>The optimal action is the action that maximize the optimal
Q-function, and optimal V-function (only one state)</p>
<p><span class="math display">\[
a_* = \arg\max_{a \in A} q(a)
\]</span></p>
<h2 id="regret-the-cost-of-exploration">Regret: The cost of
exploration</h2>
<p>In RL, the agent needs to maximize the expected cumulative discounted
reward. This means to get as much reward through the course of an
episode as soon as possible despite the environment’s stochasticity.
<em>This makes sense when the environment has multiple states and the
agent interacts with it for multiple time steps per episode. But in
MABs, while there are multiple episodes, we only have a single chance of
selecting an action in each episode.</em></p>
<p>A robust way to capture a more complete goal is for the agent to
maximize the per-episode expected reward while still minimizing the
total expected reward loss of rewards across all episodes. This value is
called <strong>total regret</strong>, denoted by <span
class="math inline">\(\tau\)</span>.</p>
<p><span class="math display">\[
\tau = \sum_{e=1}^E \Bbb{E}[v_* - q_*(A_e)]
\]</span></p>
<p>which is the expectation of the difference between the optimal value
of MAB and the true value of the action selected.</p>
<h2 id="approaches-to-solving-mab-environments">Approaches to solving
MAB environments</h2>
<p>The most popular and straight forward approach involves exploring by
injecting randomness in our action-selection process. This family of
approaches is called <strong>random exploration strategies</strong>.</p>
<p>Another approach to dealing with the exploration-exploitation dilemma
is to be optimistic. The family of <strong>optimistic exploration
strategies</strong> is a more systematic approach that quantifies the
uncertainty in the decision-making problem and increases the preference
for states with the highest uncertainty.</p>
<p>The third approach is the family of <strong>information state-space
exploration strategies</strong>.</p>
<h3 id="greedy-always-exploit">Greedy: Always exploit</h3>
<p><strong>Greedy strategy</strong>, or <strong>pure exploitation
strategy</strong>. The greedy action selction approach consists of
always selecting the action with the highest estimated value.</p>
<p><img
src="/assets/images/2023-09-13-balancing-the-gathering-and-use-of-information/pure-exploitation-in-the-bsw.png" /></p>
<p><strong>If the Q-table is initialized to zero, and there are no
negative rewards in the environment, the greedy strategy will always get
stuck with the first action.</strong></p>
<aside>
<p>💡 It is reasonable to act greedily if time is limited (one episode).
Otherwise, you can’t trade-off immediate satisfaction or reward for
gaining of information that would allow you better long-term
results.</p>
</aside>
<h3 id="random-always-explore">Random: Always explore</h3>
<p><strong>Random strategy</strong>, a <strong>pure exploration
strategy</strong>. This is simply an approach to action selection with
no exploitation at all.</p>
<p><img
src="/assets/images/2023-09-13-balancing-the-gathering-and-use-of-information/pure-exploration-in-the-bsw.png" /></p>
<aside>
<p>💡 While there’s only a single way to exploit, there are multiple
ways to explore. Exploiting is nothing but doing what you think is best.
Exploring, on the other hand, is much more complex. It’s obvious you
need to collect information. You could explore based on confidence, or
based on uncertainty.</p>
</aside>
<h3
id="epsilon-greedy-almost-always-greedy-and-sometimes-random">Epsilon-greedy:
Almost always greedy and sometimes random</h3>
<p>This hybrid strategy, <strong>epsilon greedy</strong>, consists of
acting greedily most of the time and exploring randomly every so often.
This way, the action-value function has an opportunity to converge to
its true value, which in turn, will help obtain more rewards in the long
term.</p>
<p><img
src="/assets/images/2023-09-13-balancing-the-gathering-and-use-of-information/epsilon-greedy-in-the-bsw.png" /></p>
<h3
id="decaying-epsilon-greedy-first-maximize-exploration-then-exploitation">Decaying
epsilon-greedy: First maximize exploration, then exploitation</h3>
<p>Start with a high epsilon less than or equal to one, and decay its
value on every step. This strategy, called <strong>decaying
epsilon-greedy strategy</strong>.</p>
<h3
id="optimistic-initialization-start-off-believing-its-a-wonderful-world">Optimistic
initialization: Start off believing it’s a wonderful world</h3>
<p>Treat actions that you haven’t sufficiently explored as if they were
the best possible actions —— like you’re indeed in paradise. This class
of strategies is known as <strong>optimism in the face of
uncertainty</strong>. The <strong>optimistic initialization</strong>
strategy is an instance of this class.</p>
<p><img
src="/assets/images/2023-09-13-balancing-the-gathering-and-use-of-information/optimistic-initialization-in-the-bsw.png" /></p>
<h1 id="strategic-exploration">Strategic exploration</h1>
<h2
id="softmax-select-actions-randomly-in-proportion-to-their-estimates">Softmax:
Select actions randomly in proportion to their estimates</h2>
<p>Random exploration strategies make more sense if they take into
account Q-value estimates. <strong>Softmax strategy</strong> samples an
action from a probability distribution over the action-value function
such that the probability of selecting an action is proportional over
the current action-value estimates.</p>
<p>A hyperparameter, called the <strong>temperature</strong>, can be
added to control the algorithm’s sensitiviy to the differences in
Q-value estimates.</p>
<p>The probability of selecting action <span
class="math inline">\(a\)</span> is</p>
<p><span class="math display">\[
\pi(a) = \frac{\exp\bigg[\cfrac{Q(a)}{\tau}\bigg]}{\sum^B_{b=0}
\exp\bigg[\cfrac{Q(b)}{\tau}\bigg]}
\]</span></p>
<p>where <span class="math inline">\(\tau\)</span> is the temperature
parameter.</p>
<h2 id="ucb-its-not-about-optimism-its-about-realistic-optimism">UCB:
It’s not about optimism, it’s about realistic optimism</h2>
<p>There are two inconveniences with the optimistic initialization
algorithm:</p>
<ol type="1">
<li>We don’t always know the maximum reward the agent can obtain from an
environment. If the initial Q-value is set to be much higher than the
actual maximum value, then the algorithm will perform sub-optimally
because the agent will take many episodes to bring the estimates near
the actual values. But even worse, if the Q-values are set to a value
lower than the environment’s maximum, the algorithm will no longer be
optimistic.</li>
<li>The “counts” variable in the optimistic initialization is a
hyperparameter and it needs tuning, but in reality, what we’re trying to
represent with this variable is the uncertainty of the estimate, which
shouldn’t be a hyperparameter.</li>
</ol>
<p><strong>Upper confidence bound (UCB)</strong> strategy solve the
problem. In UCB, instead of blindly hoping for the best, we look at the
uncertainty of value estimates. The more uncertain a Q-value estimate,
the more critical it is to explore it.</p>
<p><span class="math display">\[
A_e = \arg\max_a \bigg[ Q_e(a) + c \sqrt{\frac{\ln e}{N_e(a)}}\bigg]
\]</span></p>
<p>To implement the strategy, we select the action with the highest sum
of its Q-value estimate and an action-uncertainty bonus <span
class="math inline">\(U\)</span>. If we attempt action <span
class="math inline">\(a\)</span> only a few times, the <span
class="math inline">\(U\)</span> bonus is large, thus encouraging
exploring this action. Otherwise, we only add a small <span
class="math inline">\(U\)</span> bonus value to the Q-value
estimates.</p>
<h2 id="thompson-sampling-balancing-reward-and-risk">Thompson sampling:
Balancing reward and risk</h2>
<p><strong>Thompson sampling strategy</strong> is a sample-based
probability matching strategy that allows us to use Bayesian techniques
to balance the exploration and exploitation trade-off.</p>
<p><img
src="/assets/images/2023-09-13-balancing-the-gathering-and-use-of-information/comparing-two-action-value-functions.png" /></p>
<p>As the name suggests, in Thompson sampling, we sample from these
normal distributions and pick the action that returns the highest
sample. Then, to update the Gaussian distributions’ standard deviation,
we use a formula similar to the UCB strategy in which, early on when the
uncertainty is higher, the standard deviation is more significant;
therefore, the Gaussian is broad. But as the episodes progress, and the
means shift toward better and better estimates, the standard deviations
gets lower, and the Gaussian distribution shrinks, and so its samples
are more and more likely to be near the estimated mean.</p>

    </div>

    
    
    

    <footer class="post-footer">

<script>
  function resizeIframe() {
    const obj = document.querySelector('.post-body iframe');
    obj.onload = () => {
      obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
      obj.setAttribute('scrolling', 'no');
    }
  }
  resizeIframe();
</script>
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>bifnudo
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://aquietzero.github.io/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/" title="Balancing the gathering and use of information">https://aquietzero.github.io/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
              <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag"># 读书笔记</a>
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/" rel="prev" title="Balancing immediate and long-term goals">
                  <i class="fa fa-angle-left"></i> Balancing immediate and long-term goals
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/09/19/reading/grokking-deep-reinforcement-learning/2023-09-19-evaluating-agents-behaviors/" rel="next" title="Evaluating Agent's Behaviors">
                  Evaluating Agent's Behaviors <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">bifnudo</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">296k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">17:58</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/aquietzero" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"version":"11.5.0","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.0/mermaid.min.js","integrity":"sha256-K7oJiQlDulzl24ZUFOywuYme1JqBBvQzK6m8qHjt9Gk="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"aquietzero-github-io","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
