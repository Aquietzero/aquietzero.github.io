<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CAgbalumo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"aquietzero.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"width":null},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Brief of previous chapters  chapter 2: learned to represent problems in a way reinforcement learning agents can solve using Markov decision process (MDP). chapter 3: developed algorithms that solve th">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to value-based deep reinforcement learning">
<meta property="og:url" content="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/index.html">
<meta property="og:site_name" content="NullSpace">
<meta property="og:description" content="Brief of previous chapters  chapter 2: learned to represent problems in a way reinforcement learning agents can solve using Markov decision process (MDP). chapter 3: developed algorithms that solve th">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-09-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-19T07:11:29.529Z">
<meta property="article:author" content="bifnudo">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="读书笔记">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/","path":"2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/","title":"Introduction to value-based deep reinforcement learning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Introduction to value-based deep reinforcement learning | NullSpace</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BCZ3TL69CD"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-BCZ3TL69CD","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">NullSpace</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Stay hungry, stay foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">归档</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#the-kind-of-feedback-deep-reinforcement-learning-agents-use"><span class="nav-number">1.</span> <span class="nav-text">The
kind of feedback deep reinforcement learning agents use</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-reinforcement-learning-agents-deal-with-sequential-feedback"><span class="nav-number">1.1.</span> <span class="nav-text">Deep
reinforcement learning agents deal with sequential feedback</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-reinforcement-learning-agents-deal-with-evaluative-feedback"><span class="nav-number">1.2.</span> <span class="nav-text">Deep
reinforcement learning agents deal with evaluative feedback</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-reinforcement-learning-agents-deal-with-sampled-feedback"><span class="nav-number">1.3.</span> <span class="nav-text">Deep
reinforcement learning agents deal with sampled feedback</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction-to-function-approximation-for-reinforcement-learning"><span class="nav-number">2.</span> <span class="nav-text">Introduction
to function approximation for reinforcement learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#reinforcement-learning-problems-can-have-high-dimensional-state-and-action-spaces"><span class="nav-number">2.1.</span> <span class="nav-text">Reinforcement
learning problems can have high-dimensional state and action spaces</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reinforcement-learning-problems-can-have-continuous-state-and-action-spaces"><span class="nav-number">2.2.</span> <span class="nav-text">Reinforcement
learning problems can have continuous state and action spaces</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#example-a-cart-pole-environment"><span class="nav-number">3.</span> <span class="nav-text">Example: A cart-pole
environment</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#nfq-the-first-attempt-at-value-based-deep-reinforcement-learning"><span class="nav-number">4.</span> <span class="nav-text">NFQ:
The first attempt at value-based deep reinforcement learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#first-decision-point-selecting-a-value-function-to-approximate"><span class="nav-number">4.1.</span> <span class="nav-text">First
decision point: Selecting a value function to approximate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#second-decision-point-selecting-a-neural-network-architecture"><span class="nav-number">4.2.</span> <span class="nav-text">Second
decision point: Selecting a neural network architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#third-decision-point-selecting-what-to-optimize"><span class="nav-number">4.3.</span> <span class="nav-text">Third decision
point: Selecting what to optimize</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fourth-decision-point-selecting-the-targets-for-policy-evaluation"><span class="nav-number">4.4.</span> <span class="nav-text">Fourth
decision point: Selecting the targets for policy evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fifth-decision-point-selecting-an-exploration-strategy"><span class="nav-number">4.5.</span> <span class="nav-text">Fifth
decision point: Selecting an exploration strategy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sixth-decision-point-selecting-a-loss-function"><span class="nav-number">4.6.</span> <span class="nav-text">Sixth decision
point: Selecting a loss function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#seventh-decision-point-selecting-an-optimization-method"><span class="nav-number">4.7.</span> <span class="nav-text">Seventh
decision point: Selecting an optimization method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-full-algorithm"><span class="nav-number">4.8.</span> <span class="nav-text">The full algorithm</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bifnudo"
      src="/assets/images/me.webp">
  <p class="site-author-name" itemprop="name">bifnudo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/aquietzero" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;aquietzero" rel="noopener me" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.douban.com/people/aquietzero" title="豆瓣 → https:&#x2F;&#x2F;www.douban.com&#x2F;people&#x2F;aquietzero" rel="noopener me" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyunhaosss@gmail.com" title="E-Mail → mailto:zhaoyunhaosss@gmail.com" rel="noopener me" target="_blank">E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/images/me.webp">
      <meta itemprop="name" content="bifnudo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NullSpace">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Introduction to value-based deep reinforcement learning | NullSpace">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Introduction to value-based deep reinforcement learning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-25 00:00:00" itemprop="dateCreated datePublished" datetime="2023-09-25T00:00:00+08:00">2023-09-25</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>Brief of previous chapters</p>
<ul>
<li><strong>chapter 2</strong>: learned to represent problems in a way
reinforcement learning agents can solve using Markov decision process
(MDP).</li>
<li><strong>chapter 3</strong>: developed algorithms that solve these
MDPs.</li>
<li><strong>chapter 4</strong>: learned about algorithms that solve
one-step MDPs, without having access to these MDPs.</li>
<li><strong>chapter 5</strong>: explore agents that learn to evaluate
policies. Agent didn’t find optimal policies but were able to evaluate
policies and estimate value function accurately.</li>
<li><strong>chapter 6</strong>: studied agents that find optimal
policies on sequential decision-making problems under uncertainty.</li>
<li><strong>chapter 7</strong>: learned about agents that are even
better at finding optimal policies by getting the most out of their
experiences.</li>
</ul>
<p>The algorithms mentioned earlier can be referred as <strong>tabular
reinforcement learning</strong>.</p>
<h1 id="the-kind-of-feedback-deep-reinforcement-learning-agents-use">The
kind of feedback deep reinforcement learning agents use</h1>
<p>In deep reinforcement learning, we build agents that are capable of
learning from feedback that’s <strong>simultaneously
evaluative</strong>, <strong>sequential</strong>, and
<strong>sampled</strong>.</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>Sequential</strong> (as opposed to one-shot)</th>
<th><strong>Evaluative</strong> (as opposed to supervised)</th>
<th><strong>Sampled</strong> (as opposed to exhaustive)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Supervised learning</strong></td>
<td>✗</td>
<td>✗</td>
<td>✓</td>
</tr>
<tr>
<td><strong>Planning</strong> (Chapter 3)</td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td><strong>Bandits</strong> (Chapter 4)</td>
<td>✗</td>
<td>✓</td>
<td>✗</td>
</tr>
<tr>
<td><strong>Tabular reinforcement learning</strong> (Chapters 5, 6,
7)</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
</tr>
<tr>
<td><strong>Deep reinforcement learning</strong> (Chapters 8, 9, 10, 11,
12)</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
</tbody>
</table>
<h2
id="deep-reinforcement-learning-agents-deal-with-sequential-feedback">Deep
reinforcement learning agents deal with sequential feedback</h2>
<p>Deep reinforcement learning agents have to deal with sequential
feedback. One of the main challenges of sequential feedback is that your
agents can receive delayed information.</p>
<p><img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/sequential-feedback.png" /></p>
<p>The opposite of delayed feedback is immediate feedback. In supervised
learning or multi-armed bandits, decisions don’t have long-term
consequences.</p>
<h2
id="deep-reinforcement-learning-agents-deal-with-evaluative-feedback">Deep
reinforcement learning agents deal with evaluative feedback</h2>
<p>The crux of evaluative feedback is that the goodness of the feedback
is only relative, because the environment is uncertain. We don’t know
the actual dynamics of the environment, we don’t have access to the
trainsition function and reward signal.</p>
<p><img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/evaluative-feedback.png" /></p>
<p>The opposite of evaluative feedback is supervised feedback.</p>
<h2
id="deep-reinforcement-learning-agents-deal-with-sampled-feedback">Deep
reinforcement learning agents deal with sampled feedback</h2>
<p>In deep reinforcement learning, agents are unlikely to sample all
possible feedback exhaustively. Agents need to generalize using the
gathered feedback and come up with intelligent decisions based on that
generalization.</p>
<p>The opposite of sampled feedback is exhausitive feedback. To
exhaustively sample environments means agents have access to all
possible samples.</p>
<h1
id="introduction-to-function-approximation-for-reinforcement-learning">Introduction
to function approximation for reinforcement learning</h1>
<h2
id="reinforcement-learning-problems-can-have-high-dimensional-state-and-action-spaces">Reinforcement
learning problems can have high-dimensional state and action spaces</h2>
<p>The main drawback of tabular reinforcement learning is that the use
of a table to represent value functions is no longer practical in
complex problems.</p>
<h2
id="reinforcement-learning-problems-can-have-continuous-state-and-action-spaces">Reinforcement
learning problems can have continuous state and action spaces</h2>
<p>Environments can additionally have continuous variables, meaning that
a variable can take on an infinite number of values.</p>
<h1 id="example-a-cart-pole-environment">Example: A cart-pole
environment</h1>
<p><img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/cart-pole-env.png" /></p>
<p>Its state space is comprised of four variables:</p>
<ul>
<li>The cart position on the track with a range from -2.4 to 2.4</li>
<li>The cart velocity along the track with a range from -inf to inf</li>
<li>The pole angle with a range of -40 degrees to 40 degrees</li>
<li>The pole velocity at the tip with a range of -inf to inf</li>
</ul>
<p>There are two available actions in every state:</p>
<ul>
<li>Action 0 applies a -1 force to the cart (push it left)</li>
<li>Action 1 applies a +1 force to the cart (push it right)</li>
</ul>
<p>You reach a terminal state if</p>
<ul>
<li>The pole angle is more than 12 degrees away from the vertical
position</li>
<li>The cart center is more than 2.4 units from the center of the
track</li>
<li>The episode count reaches 500 time steps</li>
</ul>
<p>The reward function is</p>
<ul>
<li>+1 for every time step</li>
</ul>
<h1
id="nfq-the-first-attempt-at-value-based-deep-reinforcement-learning">NFQ:
The first attempt at value-based deep reinforcement learning</h1>
<p>The following algorithm is called <strong>neural fitted Q
(NFQ)</strong> iteration.</p>
<h2
id="first-decision-point-selecting-a-value-function-to-approximate">First
decision point: Selecting a value function to approximate</h2>
<p>To begin with, we have to choose a value function to approximate.</p>
<ul>
<li>The state-value function <span
class="math inline">\(v(s)\)</span></li>
<li>The action-value function <span class="math inline">\(q(s,
a)\)</span></li>
<li>The action-advantage function <span class="math inline">\(a(s,
a)\)</span></li>
</ul>
<p>For now, let’s settle on estimating the action-value function <span
class="math inline">\(q(s, a)\)</span>. We refer to the approximate
action-value function estimate as <span class="math inline">\(Q(s, a;
\theta)\)</span>, which mean the <span class="math inline">\(Q\)</span>
estimates are parameterized by <span
class="math inline">\(\theta\)</span>, the weights of a neural network,
a state <span class="math inline">\(s\)</span> and an action <span
class="math inline">\(a\)</span>.</p>
<h2
id="second-decision-point-selecting-a-neural-network-architecture">Second
decision point: Selecting a neural network architecture</h2>
<p>When we implemented the Q-learning agent, you noticed how the matrix
holding the action-value function was indexed by state and action pairs.
A straightforward neural network architecture is to input the state, and
the action to evaluate. The output would then be one node representing
the Q-value for that state action pair. <strong>But, a more efficient
architecture consists of only inputting the state to the neural network
and outputting the Q-values for all the actions in that
state.</strong></p>
<figure>
<img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/state-action-in-value-out-architecture.png"
alt="state-action-in-value-out-architecture" />
<figcaption
aria-hidden="true">state-action-in-value-out-architecture</figcaption>
</figure>
<figure>
<img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/state-in-values-out-architecture.png"
alt="state-in-values-out-architecture" />
<figcaption
aria-hidden="true">state-in-values-out-architecture</figcaption>
</figure>
<h2 id="third-decision-point-selecting-what-to-optimize">Third decision
point: Selecting what to optimize</h2>
<p>In the NFQ implementation, we start with a randomly initialized
action-value function. Then we evaluate the policy by sampling actions
from it. Then improve it with an exploration strategy such as
epsilon-greedy. Finally, keep iterating until we reach the desired
performance.</p>
<aside>
<p>💡 Since we don’t have access to the optimal action-value function,
and we don’t even have an optimal policy to sample from, we mush
alternate between evaluating a policy (by sampling actions from it), and
improving it (using an exploration strategy, such as
epsilon-greedy).</p>
</aside>
<h2
id="fourth-decision-point-selecting-the-targets-for-policy-evaluation">Fourth
decision point: Selecting the targets for policy evaluation</h2>
<p>There are different <strong>targets</strong> we can use for
estimating the action-value function of a policy <span
class="math inline">\(\pi\)</span>. Such as MC target, TD target, the
n-step target, or the lambda target.</p>
<p><img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/targets.png" /></p>
<p>TD targets can be on-policy, which is called the <strong>SARSA
target</strong>, or off-policy, which is called the <strong>Q-learning
target</strong>.</p>
<p><span class="math display">\[
\begin{align*}
y_i^\text{SARSA} &amp;= R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}; \theta_i)
\\
y_i^\text{Q-learning} &amp;= R_{t+1} + \gamma \max_a Q(S_{t+1}, a;
\theta_i) \\
\end{align*}
\]</span></p>
<p>The loss between a target and an estimate is</p>
<p><span class="math display">\[
L_i(\theta_i) = \Bbb E_{s, a, r, s&#39;}[(r + \gamma \max_{a&#39;}
Q(s&#39;, a&#39;; \theta_i) - Q(s, a; \theta_i))^2]
\]</span></p>
<p>where <span class="math inline">\(\Bbb E_{s, a, r, s&#39;}\)</span>
is the expectation of the experience tuple. <span
class="math inline">\(r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;;
\theta_i)\)</span> is a more general form of the Q-learning target. The
differentiate the equation.</p>
<p><span class="math display">\[
\nabla_{\theta_i} L_i(\theta_i) = \Bbb E_{s, a, r, s&#39;}[(r + \gamma
\max_{a&#39;} Q(s&#39;, a&#39;; \theta_i) - Q(s, a; \theta_i))
\color{blue} \nabla_{\theta_i} Q(s, a; \theta_i) \color{black}]
\]</span></p>
<p><strong>Notice that the gradient must only go through the predicted
value.</strong></p>
<h2 id="fifth-decision-point-selecting-an-exploration-strategy">Fifth
decision point: Selecting an exploration strategy</h2>
<p>One interesting fact of off-policy learning algorithms is that the
policy generating behavior can be virtually anything. That is, it can be
anything as long as it has broad support, which means it must ensure
enough exploration of all state-action pairs.</p>
<h2 id="sixth-decision-point-selecting-a-loss-function">Sixth decision
point: Selecting a loss function</h2>
<p><strong>Mean squared error (L2 loss)</strong> is a common loss
function.</p>
<p><img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/circular-dependency-of-the-action-value-function.png" /></p>
<h2 id="seventh-decision-point-selecting-an-optimization-method">Seventh
decision point: Selecting an optimization method</h2>
<p>Gradient descent is a stable optimization method given a couple of
assumptions: data must be independent and identically distributed (IID),
and targets must be stationary. In reinforcement learning, however, we
cannot ensure any of these assumptions hold, so choosing a robust
optimization method to minimize the loss function can often make the
difference between convergence and divergence.</p>
<h2 id="the-full-algorithm">The full algorithm</h2>
<p>Selections we made</p>
<ul>
<li>Approximate the action-value function <span
class="math inline">\(Q(s, a; \theta)\)</span></li>
<li>Use a state-in-values-out architecture (nodes: 4, 512, 128, 2)</li>
<li>Optimize the action-value function to approximate the optimal
action-value function <span class="math inline">\(q^*(s,
a)\)</span></li>
<li>Use off-policy TD targets <span class="math inline">\((r + \gamma *
\max_a&#39; Q(s&#39;, a&#39;; \theta))\)</span> to evaluate
policies</li>
<li>Use an epsilon-greedy (<span class="math inline">\(\epsilon =
0.5\)</span>) strategy to improve policies</li>
<li>Use MSE for loss function</li>
<li>Use RMSprop as the optimizer with a learning rate of 0.0005</li>
</ul>
<p>NFQ has three main steps:</p>
<ol type="1">
<li>Collect experiences tupes <span class="math inline">\((s, a, r,
s&#39;, d)\)</span></li>
<li>Calculate the off-policy TD targets <span class="math inline">\((r +
\gamma * \max_{a&#39;} Q(s&#39;, a&#39;; \theta))\)</span></li>
<li>Fit the action-value function <span class="math inline">\(Q(s,
a;\theta)\)</span> using MSE and RMSprop</li>
</ol>
<p>Repeats step 2 and 3 <span class="math inline">\(K ( = 40)\)</span>
times before going back to step 1.</p>
<p><img
src="/assets/images/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/nfq.png" /></p>

    </div>

    
    
    

    <footer class="post-footer">

<script>
  function resizeIframe() {
    const obj = document.querySelector('.post-body iframe');
    obj.onload = () => {
      obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
      obj.setAttribute('scrolling', 'no');
    }
  }
  resizeIframe();
</script>
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>bifnudo
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/" title="Introduction to value-based deep reinforcement learning">https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
              <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag"># 读书笔记</a>
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/" rel="prev" title="More stable value-based methods">
                  <i class="fa fa-angle-left"></i> More stable value-based methods
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/10/10/reading/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2023-10-12-gated-rnn/" rel="next" title="Gated RNN">
                  Gated RNN <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">bifnudo</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">305k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">18:28</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/aquietzero" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"version":"11.5.0","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.0/mermaid.min.js","integrity":"sha256-K7oJiQlDulzl24ZUFOywuYme1JqBBvQzK6m8qHjt9Gk="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"aquietzero-github-io","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
