<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CAgbalumo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"aquietzero.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"width":null},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"æœç´¢...","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="DQN: Making reinforcement learning more like supervised learning Deep Q-network (DQN) is one of the most popular DRL algorithms because it started a series of research innovations that mark the histor">
<meta property="og:type" content="article">
<meta property="og:title" content="More stable value-based methods">
<meta property="og:url" content="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/index.html">
<meta property="og:site_name" content="NullSpace">
<meta property="og:description" content="DQN: Making reinforcement learning more like supervised learning Deep Q-network (DQN) is one of the most popular DRL algorithms because it started a series of research innovations that mark the histor">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-09-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-19T07:11:29.529Z">
<meta property="article:author" content="bifnudo">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="å¼ºåŒ–å­¦ä¹ ">
<meta property="article:tag" content="è¯»ä¹¦ç¬”è®°">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/","path":"2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/","title":"More stable value-based methods"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>More stable value-based methods | NullSpace</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BCZ3TL69CD"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-BCZ3TL69CD","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ " role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">NullSpace</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Stay hungry, stay foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="æœç´¢" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">é¦–é¡µ</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">æ ‡ç­¾</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">åˆ†ç±»</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">å½’æ¡£</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">å…³äº</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#dqn-making-reinforcement-learning-more-like-supervised-learning"><span class="nav-number">1.</span> <span class="nav-text">DQN:
Making reinforcement learning more like supervised learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#common-problems-in-value-based-deep-reinforcement-learning"><span class="nav-number">1.1.</span> <span class="nav-text">Common
problems in value-based deep reinforcement learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-violation-of-the-iid-assumption"><span class="nav-number">1.1.1.</span> <span class="nav-text">The violation of the IID
assumption</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stationary-of-targets"><span class="nav-number">1.1.2.</span> <span class="nav-text">Stationary of targets</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#using-target-networks"><span class="nav-number">1.2.</span> <span class="nav-text">Using target networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#using-larger-networks"><span class="nav-number">1.3.</span> <span class="nav-text">Using larger networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#using-experience-replay"><span class="nav-number">1.4.</span> <span class="nav-text">Using experience replay</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-full-deep-q-network-dqn-algorithm"><span class="nav-number">1.5.</span> <span class="nav-text">The full deep Q-network
(DQN) algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#double-dqn-mitigating-the-overestimation-of-action-value-functions"><span class="nav-number">2.</span> <span class="nav-text">Double
DQN: Mitigating the overestimation of action-value functions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#separating-action-selection-from-action-evaluation"><span class="nav-number">2.1.</span> <span class="nav-text">Separating
action selection from action evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a-more-forgiving-loss-function"><span class="nav-number">2.2.</span> <span class="nav-text">A more forgiving loss
function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-full-double-deep-q-network-ddqn-algorithm"><span class="nav-number">2.3.</span> <span class="nav-text">The full double
deep Q-network (DDQN) algorithm</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bifnudo"
      src="/assets/images/me.webp">
  <p class="site-author-name" itemprop="name">bifnudo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">150</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">81</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/aquietzero" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;aquietzero" rel="noopener me" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.douban.com/people/aquietzero" title="è±†ç“£ â†’ https:&#x2F;&#x2F;www.douban.com&#x2F;people&#x2F;aquietzero" rel="noopener me" target="_blank">è±†ç“£</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyunhaosss@gmail.com" title="E-Mail â†’ mailto:zhaoyunhaosss@gmail.com" rel="noopener me" target="_blank">E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/images/me.webp">
      <meta itemprop="name" content="bifnudo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NullSpace">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="More stable value-based methods | NullSpace">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          More stable value-based methods
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">å‘è¡¨äº</span>

      <time title="åˆ›å»ºæ—¶é—´ï¼š2023-09-25 00:00:00" itemprop="dateCreated datePublished" datetime="2023-09-25T00:00:00+08:00">2023-09-25</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqusï¼š</span>
    
    <a title="disqus" href="/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
      <span>1.6k</span>
    </span>
    <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
      <span>6 åˆ†é’Ÿ</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1
id="dqn-making-reinforcement-learning-more-like-supervised-learning">DQN:
Making reinforcement learning more like supervised learning</h1>
<p>Deep Q-network (DQN) is one of the most popular DRL algorithms
because it started a series of research innovations that mark the
history of RL.</p>
<h2
id="common-problems-in-value-based-deep-reinforcement-learning">Common
problems in value-based deep reinforcement learning</h2>
<p>Two most common problems in value-based deep reinforcement
learning:</p>
<h3 id="the-violation-of-the-iid-assumption">The violation of the IID
assumption</h3>
<p>In supervised learning, we obtain a full dataset in advance. We
preprocess it, shuffle it, and then split it into sets for training. One
crucial step in this process is the shuffling of the dataset. By doing
so, we allow our optimization method to avoid developing overfitting
biases.</p>
<p>In reinforcement learning, data is often gathered online; as a
result, the experience sample generated at time step <span
class="math inline">\(t+1\)</span> correlates with experience sample
gathered at time step <span class="math inline">\(t\)</span>. Moreover,
as the policy is to improve, it changes the underlying data-generating
process changes, which means that <strong>new data is locally correlated
and not evenly distributed</strong>.</p>
<h3 id="stationary-of-targets">Stationary of targets</h3>
<p>The targets we use to train our network are calculated using the
network itself. As a result, the function changes with every update, in
turn changing the targets.</p>
<p><img
src="/assets/images/2023-09-25-more-stable-value-based-methods/non-stationarity-of-targets.png" /></p>
<h2 id="using-target-networks">Using target networks</h2>
<p>A straightforward way to make target values more stationary is to
have a separate network that we can fix for multiple steps and reserve
it for calculating more stationary targets. The network with this
purpose in DQN is called the <strong>target network</strong>.</p>
<p><img
src="/assets/images/2023-09-25-more-stable-value-based-methods/q-function-optimization-without-target-network.png" /></p>
<p>By using a target network to fix targets, we mitigate the issue of
â€œchasing your own tailâ€ by artificially creating several small
supervised learning problems presented sequentially to the agent.</p>
<p><span class="math display">\[
\begin{gather}
\nabla_{\theta_i} L_i(\theta_i) = \Bbb E_{s, a, r, s&#39;}[(r + \gamma
\max_{a&#39;} Q(s&#39;, a&#39;; \theta_i) - Q(s, a;
\theta_i))  \nabla_{\theta_i} Q(s, a; \theta_i)] \\
\downarrow \\
\nabla_{\theta_i} L_i(\theta_i) = \Bbb E_{s, a, r, s&#39;}[(r + \gamma
\max_{a&#39;} Q(s&#39;, a&#39;; \textcolor{blue}{\theta^-}) - Q(s, a;
\theta_i)) \nabla_{\theta_i} Q(s, a; \theta_i)]
\end{gather}
\]</span></p>
<p>The only difference between these two equations is the age of the
neural network weights. A target network is a previous instance of the
neural network that we freeze for a number of steps. <strong>In
practice, we donâ€™t have two â€œnetworksâ€, we have two instances of the
neural network weights</strong>.</p>
<p>By using target networks, we stablize training, but we also slow down
learning because youâ€™re no longer training on up-to-date values.</p>
<h2 id="using-larger-networks">Using larger networks</h2>
<p>Another way to lessen the non-stationarity issue, to some degree, is
to use larger networks.</p>
<aside>
<p>ğŸ’¡ To mitigate the non-stationary issue we can 1. Create a target
network that provides us with a temporarily stationary target value. 2.
Create large-enough networks so that they can â€œseeâ€ the small
differences between similar states (like those temporally
correlated)</p>
</aside>
<h2 id="using-experience-replay">Using experience replay</h2>
<p>Experience replay consists of a data structure, often referred to as
a replay buffer or a replay memory, that holds experience samples for
several steps, allowing the sampling of mini-batches from a broad set of
past experiences.</p>
<p><img
src="/assets/images/2023-09-25-more-stable-value-based-methods/dqn-with-replay-buffer.png" /></p>
<p>Unfortunately, the implementation becomes a little bit of challenge
when working with high-dimensional observations, because poorly
implemented replay buffers hit a hardware memory limit quickly in
high-dimensional environments.</p>
<p><span class="math display">\[
\begin{gather}
\nabla_{\theta_i} L_i(\theta_i) = \Bbb E_{s, a, r, s&#39;}[(r + \gamma
\max_{a&#39;} Q(s&#39;, a&#39;; {\theta^-}) - Q(s, a; \theta_i))
\nabla_{\theta_i} Q(s, a; \theta_i)] \\
\downarrow \\
\nabla_{\theta_i} L_i(\theta_i) = \Bbb E_{\textcolor{blue}{(s, a, r,
s&#39;) \sim U(D)}}[(r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;;
{\theta^-}) - Q(s, a; \theta_i)) \nabla_{\theta_i} Q(s, a; \theta_i)]
\end{gather}
\]</span></p>
<p>The only difference between these two equations is that weâ€™re now
obtaining the experiences we use for training by <strong>sampling
uniformly at ranom the replay buffer <span
class="math inline">\(D\)</span>, instead of using the online
experiences as before</strong>.</p>
<aside>
<p>ğŸ’¡ The best solution to the problem of data not being IID is called
experience replay. As the agent collects experiences tuples <span
class="math inline">\(e_t = (S_t, A_t, R_{t+1}, S_{t+1})\)</span>
online, we insert them into a data structure, commonly referred to as
the replay buffer <span class="math inline">\(D\)</span>, such that
<span class="math inline">\(D = \{e_1, e_2, \dots, e_t\}\)</span>. The
size of the replay buffer depends on the problem. Then the agent is
trained on mini-batches sampled, usually uniformly at random, from the
buffer, so that each sample has equal probability of being selected.</p>
</aside>
<h2 id="the-full-deep-q-network-dqn-algorithm">The full deep Q-network
(DQN) algorithm</h2>
<p>Selections we made</p>
<ul>
<li>Approximate the action-value function <span
class="math inline">\(Q(s, a; \theta)\)</span></li>
<li>Use a state-in-values-out architecture (nodes: 4, 512, 128, 2)</li>
<li>Optimize the action-value function to approximate the optimal
action-value function <span class="math inline">\(q^*(s,
a)\)</span></li>
<li>Use off-policy TD targets <span class="math inline">\((r + \gamma *
\max_a&#39; Q(s&#39;, a&#39;; \theta))\)</span> to evaluate
policies</li>
<li>Use MSE for loss function</li>
<li>Use RMSprop as the optimizer with a learning rate of 0.0005</li>
</ul>
<p>Some of the differences are that in the DQN implementation we now</p>
<ul>
<li>Use an exponentially decaying epsilon-greedy strategy to improve
policies, decaying from <span class="math inline">\(1.0\)</span> to
<span class="math inline">\(0.3\)</span> in roughly <span
class="math inline">\(20,000\)</span> steps.</li>
<li>Use a replay buffer with <span class="math inline">\(320\)</span>
samples min, <span class="math inline">\(50,000\)</span> max, and
mini-batches of <span class="math inline">\(64\)</span>.</li>
<li>Use a target network that updates every <span
class="math inline">\(15\)</span> steps.</li>
</ul>
<p>DQN has three main steps:</p>
<ol type="1">
<li>Collect experiences tupes <span class="math inline">\((s, a, r,
s&#39;, d)\)</span> and insert it into the replay buffer</li>
<li>Randomly sample a mini-batch from the buffer, and calculate the
off-policy TD targets <span class="math inline">\((r + \gamma *
\max_{a&#39;} Q(s&#39;, a&#39;; \theta))\)</span> for the whole
batch</li>
<li>Fit the action-value function <span class="math inline">\(Q(s,
a;\theta)\)</span> using MSE and RMSprop</li>
</ol>
<h1
id="double-dqn-mitigating-the-overestimation-of-action-value-functions">Double
DQN: Mitigating the overestimation of action-value functions</h1>
<p>One of the main improvements to DQN is called <strong>double deep
Q-networks (DDQN)</strong>. This improvement consists of adding double
learning to our DQN agent.</p>
<h2 id="separating-action-selection-from-action-evaluation">Separating
action selection from action evaluation</h2>
<p>One way to better understand positive bias and how we can address it
when using function approximation is by unwrapping the <span
class="math inline">\(**\max**\)</span> operator in the target
calculations. <strong>The <span class="math inline">\(\max\)</span> of a
Q-function is the same as the Q-function of the <span
class="math inline">\(\arg\max\)</span> action.</strong></p>
<p><span class="math display">\[
\max_{a&#39;} Q(s&#39;, a&#39;; \theta^-) \iff Q(s&#39;,
\arg\max_{a&#39;} Q(s&#39;, a&#39;; \theta^-); \theta^-)
\]</span></p>
<p>plugging this equivalence into the DQN function, we have</p>
<p><span class="math display">\[
\nabla_{\theta_i} L_i(\theta_i) =
\Bbb E_{
    {(s, a, r, s&#39;) \sim U(D)}
}[(r + \gamma \textcolor{blue}{Q(s&#39;, \arg\max_{a&#39;} Q(s&#39;,
a&#39;; \theta^-); \theta^-)} - Q(s, a; \theta_i)) \nabla_{\theta_i}
Q(s, a; \theta_i)]
\]</span></p>
<p>A way to reduce the chance of positive bias is to have two instances
of the action-value function. <strong>In double learning, one estimator
selects the index of what is believes to be the highest-valued action,
and the other estimator gives the value of this action.</strong></p>
<p>In practice, we can perform double learning with the other network,
the target network. However, instead of training both the online and
target networks, we continue training only the online network, but use
the target network to help us cross-validate the estimates. <strong>We
use the online network to find the index of the best action, then we use
the target network to evaluate the previously selected
action.</strong></p>
<p><img
src="/assets/images/2023-09-25-more-stable-value-based-methods/selecting-action-evaluating-action.png" /></p>
<p><span class="math display">\[
\nabla_{\theta_i} L_i(\theta_i) = \Bbb E_{
    {(s, a, r, s&#39;) \sim U(D)}
}[(r + \gamma \textcolor{blue}{Q(s&#39;, \arg\max_{a&#39;} Q(s&#39;,
a&#39;; \textcolor{red}{\theta_i}); \theta^-)} - Q(s, a; \theta_i))
\nabla_{\theta_i} Q(s, a; \theta_i)]
\]</span></p>
<p>The only difference in DDQN is now we use the online weights to
select the action, but still use the frozen weights to get the
estimates.</p>
<h2 id="a-more-forgiving-loss-function">A more forgiving loss
function</h2>
<p>MSE is a ubiquitous loss fucntion because itâ€™s simple, but one of the
issues with using MSE for reinforcement learning is that <strong>it
penalizes large errors more than small errors.</strong></p>
<p><img
src="/assets/images/2023-09-25-more-stable-value-based-methods/mse.png" /></p>
<p>A more robust to outliers is the <strong>mean absolute
error</strong>, also know as <strong>MAE</strong> or <strong>L1
loss</strong>.</p>
<p><img
src="/assets/images/2023-09-25-more-stable-value-based-methods/mae.png" /></p>
<p>But MAE does not have its gradients decrease as the loss goes to
zero. Combing MSE and MAE we have <strong>Huber loss</strong>. The Huber
loss uses a hyperparameter <span class="math inline">\(\delta\)</span>
to set the threshold in which the loss goes from quadratic to
linear.</p>
<p><img
src="/assets/images/2023-09-25-more-stable-value-based-methods/mse-mae-huber-loss.png" /></p>
<p>When <span class="math inline">\(\delta = 0\)</span>, it equals to
MAE while when <span class="math inline">\(\delta = \infty\)</span> itâ€™s
MSE.</p>
<h2 id="the-full-double-deep-q-network-ddqn-algorithm">The full double
deep Q-network (DDQN) algorithm</h2>
<p>Selections we made</p>
<ul>
<li>Approximate the action-value function <span
class="math inline">\(Q(s, a; \theta)\)</span></li>
<li>Use a state-in-values-out architecture (nodes: 4, 512, 128, 2)</li>
<li>Optimize the action-value function to approximate the optimal
action-value function <span class="math inline">\(q^*(s,
a)\)</span></li>
<li>Use off-policy TD targets <span class="math inline">\((r + \gamma *
\max_a&#39; Q(s&#39;, a&#39;; \theta))\)</span> to evaluate
policies</li>
<li>Use an adjustable Huber loss and set the
<code>max_gradient_norm</code> to <span
class="math inline">\(\infty\)</span>, which is MSE</li>
<li>Use RMSprop as the optimizer with a learning rate of 0.0007</li>
</ul>
<p>In DDQN, weâ€™re still using</p>
<ul>
<li>An exponentially decaying epsilon-greedy strategy to improve
policies, decaying from <span class="math inline">\(1.0\)</span> to
<span class="math inline">\(0.3\)</span> in roughly <span
class="math inline">\(20,000\)</span> steps.</li>
<li>Use a replay buffer with <span class="math inline">\(320\)</span>
samples min, <span class="math inline">\(50,000\)</span> max, and
mini-batches of <span class="math inline">\(64\)</span>.</li>
<li>Use a target network that updates every <span
class="math inline">\(15\)</span> steps.</li>
</ul>
<p>DDQN has three main steps:</p>
<ol type="1">
<li>Collect experiences tupes <span class="math inline">\((s, a, r,
s&#39;, d)\)</span> and insert it into the replay buffer</li>
<li>Randomly sample a mini-batch from the buffer, and calculate the
off-policy TD targets <span class="math inline">\((r + \gamma *
\max_{a&#39;} Q(s&#39;, a&#39;; \theta))\)</span> for the whole
batch</li>
<li>Fit the action-value function <span class="math inline">\(Q(s,
a;\theta)\)</span> using MSE and RMSprop</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

<script>
  function resizeIframe() {
    const obj = document.querySelector('.post-body iframe');
    obj.onload = () => {
      obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
      obj.setAttribute('scrolling', 'no');
    }
  }
  resizeIframe();
</script>
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>æœ¬æ–‡ä½œè€…ï¼š </strong>bifnudo
  </li>
  <li class="post-copyright-link">
      <strong>æœ¬æ–‡é“¾æ¥ï¼š</strong>
      <a href="https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/" title="More stable value-based methods">https://aquietzero.github.io/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-more-stable-value-based-methods/</a>
  </li>
  <li class="post-copyright-license">
      <strong>ç‰ˆæƒå£°æ˜ï¼š </strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ï¼
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># å¼ºåŒ–å­¦ä¹ </a>
              <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag"># è¯»ä¹¦ç¬”è®°</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/22/reading/grokking-deep-reinforcement-learning/2023-09-22-achieving-goals-more-effectively-and-efficiently/" rel="prev" title="Achieving goals more effectively and efficiently">
                  <i class="fa fa-angle-left"></i> Achieving goals more effectively and efficiently
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/09/25/reading/grokking-deep-reinforcement-learning/2023-09-25-introduction-to-value-based-deep-reinforcement-learning/" rel="next" title="Introduction to value-based deep reinforcement learning">
                  Introduction to value-based deep reinforcement learning <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">bifnudo</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="ç«™ç‚¹æ€»å­—æ•°">350k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="ç«™ç‚¹é˜…è¯»æ—¶é•¿">21:11</span>
  </span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="è¿”å›é¡¶éƒ¨">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/aquietzero" class="github-corner" title="åœ¨ GitHub ä¸Šå…³æ³¨æˆ‘" aria-label="åœ¨ GitHub ä¸Šå…³æ³¨æˆ‘" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"version":"11.5.0","config":{"flowchart":{"curve":"basis"}},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.0/mermaid.min.js","integrity":"sha256-K7oJiQlDulzl24ZUFOywuYme1JqBBvQzK6m8qHjt9Gk="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"aquietzero-github-io","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
