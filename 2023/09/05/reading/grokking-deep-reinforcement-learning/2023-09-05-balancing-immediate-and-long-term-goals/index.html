<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CAgbalumo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"aquietzero.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"width":null},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="The objective of a decision-making agent At first, it seems the agent’s goal is to find a sequence of actions that will maximize the return: the sum of rewards during an episode or the entire life of">
<meta property="og:type" content="article">
<meta property="og:title" content="Balancing immediate and long-term goals">
<meta property="og:url" content="https://aquietzero.github.io/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/index.html">
<meta property="og:site_name" content="NullSpace">
<meta property="og:description" content="The objective of a decision-making agent At first, it seems the agent’s goal is to find a sequence of actions that will maximize the return: the sum of rewards during an episode or the entire life of">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-09-04T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-19T07:11:29.528Z">
<meta property="article:author" content="bifnudo">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="读书笔记">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://aquietzero.github.io/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://aquietzero.github.io/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/","path":"2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/","title":"Balancing immediate and long-term goals"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Balancing immediate and long-term goals | NullSpace</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BCZ3TL69CD"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-BCZ3TL69CD","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">NullSpace</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Stay hungry, stay foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">归档</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-objective-of-a-decision-making-agent"><span class="nav-number">1.</span> <span class="nav-text">The objective of a
decision-making agent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#policies-per-state-action-prescriptions"><span class="nav-number">1.1.</span> <span class="nav-text">Policies: Per-state
action prescriptions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#state-value-function-what-to-expect-from-here"><span class="nav-number">1.2.</span> <span class="nav-text">State-value
function: What to expect from here?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#action-value-function-what-should-i-expect-from-here-if-i-do-this"><span class="nav-number">1.3.</span> <span class="nav-text">Action-value
function: What should I expect from here if I do this?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#action-advantage-function-how-much-better-if-i-do-that"><span class="nav-number">1.4.</span> <span class="nav-text">Action-advantage
function: How much better if I do that?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimality"><span class="nav-number">1.5.</span> <span class="nav-text">Optimality</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#planning-optimal-sequences-of-actions"><span class="nav-number">2.</span> <span class="nav-text">Planning optimal
sequences of actions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-evaluation-rating-policies"><span class="nav-number">2.1.</span> <span class="nav-text">Policy evaluation: Rating
policies</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#policy-evaluation-equation"><span class="nav-number">2.1.1.</span> <span class="nav-text">Policy-evaluation equation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-improvement-using-ratings-to-get-better"><span class="nav-number">2.2.</span> <span class="nav-text">Policy
improvement: Using ratings to get better</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-iteration-pi"><span class="nav-number">2.3.</span> <span class="nav-text">Policy iteration (PI)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#value-iteration-vi-improving-behaviors-early"><span class="nav-number">2.4.</span> <span class="nav-text">Value iteration
(VI): Improving behaviors early</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bifnudo"
      src="/assets/images/me.webp">
  <p class="site-author-name" itemprop="name">bifnudo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">124</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/aquietzero" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;aquietzero" rel="noopener me" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.douban.com/people/aquietzero" title="豆瓣 → https:&#x2F;&#x2F;www.douban.com&#x2F;people&#x2F;aquietzero" rel="noopener me" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyunhaosss@gmail.com" title="E-Mail → mailto:zhaoyunhaosss@gmail.com" rel="noopener me" target="_blank">E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://aquietzero.github.io/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/images/me.webp">
      <meta itemprop="name" content="bifnudo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NullSpace">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Balancing immediate and long-term goals | NullSpace">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Balancing immediate and long-term goals
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-05 00:00:00" itemprop="dateCreated datePublished" datetime="2023-09-05T00:00:00+08:00">2023-09-05</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="the-objective-of-a-decision-making-agent">The objective of a
decision-making agent</h2>
<p>At first, it seems the agent’s goal is to find a sequence of actions
that will maximize the return: the sum of rewards during an episode or
the entire life of the agent. <strong>But, plans are not enough in
stochastic environments. What the agent needs to come up with is called
policy.</strong></p>
<p>Policies are universal plans, they cover all possible states.</p>
<figure>
<img
src="/assets/images/2023-09-05-balancing-immediate-and-long-term-goals/optimal-policy-in-the-swf-environtment.png"
alt="optimal-policy-in-the-swf-environtment" />
<figcaption
aria-hidden="true">optimal-policy-in-the-swf-environtment</figcaption>
</figure>
<h3 id="policies-per-state-action-prescriptions">Policies: Per-state
action prescriptions</h3>
<p>Given the environment, the agent needs to find a
<strong>policy</strong>, denoted as <span
class="math inline">\(\pi\)</span>. A policy is a function that
prescribes actions to take for a given nonterminal state.</p>
<h3 id="state-value-function-what-to-expect-from-here">State-value
function: What to expect from here?</h3>
<p>If a policy and the MDP is given, we would be able to calculate the
expected return starting from every single state. The value of a state
<span class="math inline">\(s\)</span> under policy <span
class="math inline">\(\pi\)</span> is the expectation of returns if the
agent follows policy <span class="math inline">\(\pi\)</span> starting
from state <span class="math inline">\(s\)</span>. Calculate this for
every state, and you get the <strong>state-value function</strong>, or
<strong>V-function</strong>.</p>
<p><span class="math display">\[
\begin{align*}
\blue{v_\pi(s)} &amp;= \Bbb{E}_\pi [G_t | S_t = s] \\
&amp;= \Bbb{E}_\pi [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
| S_t = s] \\
&amp;= \Bbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&amp;= \sum_a \pi(a|s) \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma
\blue{v_\pi(s&#39;)}], \forall s \in S
\end{align*}
\]</span></p>
<p>The above function is call <strong>Bellman equation</strong>. Note
that the equation is given in recursive form.</p>
<h3
id="action-value-function-what-should-i-expect-from-here-if-i-do-this">Action-value
function: What should I expect from here if I do this?</h3>
<p>Another critical question that we often need to ask is the value of
taking action <span class="math inline">\(a\)</span> in a state <span
class="math inline">\(s\)</span>. The <strong>action-value
function</strong>, also known as <strong>Q-function</strong> or <span
class="math inline">\(Q_\pi (s, a)\)</span>, captures the expected
return if the agent follows policy <span
class="math inline">\(\pi\)</span> after taking action <span
class="math inline">\(a\)</span> in state <span
class="math inline">\(s\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
q_\pi(s, a) &amp;= \Bbb{E}_\pi[G_t | S_t = s, A_t = a] \\
&amp;= \Bbb{E}_\pi[R_t + \gamma G_{t+1} | S_t = s, A_t = a] \\
&amp;= \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma v_\pi(s&#39;)],
\forall s \in S, \forall a \in A
\end{align*}
\]</span></p>
<p>The above function is the <strong>Bellman equation for action
values</strong>.</p>
<h3
id="action-advantage-function-how-much-better-if-i-do-that">Action-advantage
function: How much better if I do that?</h3>
<p>The advantage function, also known as the A-function, or <span
class="math inline">\(A_\pi(s, a)\)</span>, is the difference between
the action-value function of action <span
class="math inline">\(a\)</span> in state <span
class="math inline">\(s\)</span> and the state-value function of state
<span class="math inline">\(s\)</span> under policy <span
class="math inline">\(\pi\)</span>.</p>
<p><span class="math display">\[
a_\pi(s, a) = q_\pi(s, a) - v_\pi(s)
\]</span></p>
<p>The advantage function describes how much better it is to take action
<span class="math inline">\(a\)</span> instead of following policy <span
class="math inline">\(\pi\)</span>.</p>
<h3 id="optimality">Optimality</h3>
<p><strong>Policies</strong>, <strong>state-value functions</strong>,
<strong>action-value functions</strong>, and <strong>action-advantage
functions</strong> are the components we use to describe, evaluate, and
improve behaviors. We call it <strong>optimality</strong> when these
components are the best they can be.</p>
<p>An <strong>optimal policy</strong> is a policy that for every state
can obtain expected returns greater than or equal to any other
policy.</p>
<p><span class="math display">\[
\begin{align*}
v_*(s) &amp;= \max_\pi v_\pi(s), \forall s \in S \\
q_*(s, a) &amp;= \max_\pi q_\pi(s, a), \forall s \in S, \forall a \in
A(s) \\
v_*(s) &amp;= \max_a \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma
v_*(s&#39;)] \\
q_*(s, a) &amp;= \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma
\max_a&#39; q_*(s&#39;, a&#39;)]
\end{align*}
\]</span></p>
<h2 id="planning-optimal-sequences-of-actions">Planning optimal
sequences of actions</h2>
<h3 id="policy-evaluation-rating-policies">Policy evaluation: Rating
policies</h3>
<p>The policy-evaluation algorithm consists of calculating the
V-function for a given policy by sweeping through the state space and
iteratively improving estimates.</p>
<h4 id="policy-evaluation-equation">Policy-evaluation equation</h4>
<p>The policy-evaluation algorithm consist of the iterative
approximation of the state-value function of the policy under
evaluation. The algorithm converges as <span
class="math inline">\(k\)</span> approaches infinity.</p>
<p>Initialize <span class="math inline">\(v_0(s)\)</span> for all <span
class="math inline">\(s\)</span> in <span
class="math inline">\(S\)</span> arbitrarily, and to <span
class="math inline">\(0\)</span> is <span
class="math inline">\(s\)</span> is terminal. Then, increase <span
class="math inline">\(k\)</span> and iteratively improve the estimates
by following the equation below.</p>
<p><span class="math display">\[
v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s&#39;, r} p(s&#39;, r|s, a)[r +
\gamma v_k(s&#39;)]
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_evaluation</span>(<span class="params">pi, P, gamma=<span class="number">1.0</span>, theta=<span class="number">1e-10</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  pi: policy</span></span><br><span class="line"><span class="string">  P: MDP</span></span><br><span class="line"><span class="string">  gamma: discount factor</span></span><br><span class="line"><span class="string">  theta: use to check convergence</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  prev_V = np.zeros(<span class="built_in">len</span>(P))</span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    V = np.zeros(<span class="built_in">len</span>(P))</span><br><span class="line">    <span class="comment"># loop over the states</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(P)):</span><br><span class="line">  	  <span class="comment"># P[s][pi(s)] returns possible transitions</span></span><br><span class="line">  	  <span class="comment"># probability, next state, reward, and a done flag</span></span><br><span class="line">  	  <span class="keyword">for</span> prob, next_state, reward, done <span class="keyword">in</span> P[s][pi(s)]:</span><br><span class="line">        V[s] += prob * (reward + gamma * prev_V[next_state] * (<span class="keyword">not</span> done))</span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(prev_V - V)) &lt; theta:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line">    prev_V = V.copy()</span><br><span class="line">  <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure>
<figure>
<img
src="/assets/images/2023-09-05-balancing-immediate-and-long-term-goals/results-of-policy-evolution.png"
alt="results-of-policy-evolution.png" />
<figcaption
aria-hidden="true">results-of-policy-evolution.png</figcaption>
</figure>
<h3 id="policy-improvement-using-ratings-to-get-better">Policy
improvement: Using ratings to get better</h3>
<p>Using the V-function and the MDP, you get an estimate of the
Q-function. The Q-function will give you a glimpse of the values of all
actions for all states, and these values, can hint at how to improve
policies.</p>
<figure>
<img
src="/assets/images/2023-09-05-balancing-immediate-and-long-term-goals/how-can-q-function-help-us-improve-policies.png"
alt="how-can-q-function-help-us-improve-policies.png" />
<figcaption
aria-hidden="true">how-can-q-function-help-us-improve-policies.png</figcaption>
</figure>
<figure>
<img
src="/assets/images/2023-09-05-balancing-immediate-and-long-term-goals/state-value-function-of-the-careful-policy.png"
alt="state-value-function-of-the-careful-policy.png" />
<figcaption
aria-hidden="true">state-value-function-of-the-careful-policy.png</figcaption>
</figure>
<p>We used the state-value function of the original policy and the MDP
to calculate its action-value function. Then, acting greedily with
respect to the action-value function gave us an improved policy.</p>
<p><span class="math display">\[
\pi&#39;(s) = \argmax_a \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma
v_\pi(s&#39;)]
\]</span></p>
<p>New policy <span class="math inline">\(\pi&#39;\)</span> is obtained
by taking the highest-valued action. The valued action is calculated by
the weighted sum of all rewards and values of all possible next
states.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_improvement</span>(<span class="params">V, P, gamma=<span class="number">1.0</span></span>):</span><br><span class="line">  Q = np.zeros((<span class="built_in">len</span>(P), <span class="built_in">len</span>(P[<span class="number">0</span>])), dtype=np.float64)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># loop through the states, actions, and transitions</span></span><br><span class="line">  <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(P)):</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(P[s])):</span><br><span class="line">  	  <span class="keyword">for</span> prob, next_state, reward, done <span class="keyword">in</span> P[s][a]:</span><br><span class="line">        Q[s][a] += prob * (reward + gamma * V[next_state] * (<span class="keyword">not</span> done))</span><br><span class="line">  </span><br><span class="line">  new_pi = <span class="keyword">lambda</span> s: &#123; s: a <span class="keyword">for</span> s, a <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.argmax(Q, axis=<span class="number">1</span>)) &#125;[s]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> new_pi</span><br></pre></td></tr></table></figure>
<h3 id="policy-iteration-pi">Policy iteration (PI)</h3>
<p>Policy iteration algorithm starts with random policy, then use the
V-function and Q-function to improve the policy.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_iteration</span>(<span class="params">P, gamma=<span class="number">1.0</span>, theta=<span class="number">1e-10</span></span>):</span><br><span class="line">  <span class="comment"># randomly generate a policy</span></span><br><span class="line">  random_actions = np.random.choice(<span class="built_in">tuple</span>(P[<span class="number">0</span>].keys()), <span class="built_in">len</span>(P))</span><br><span class="line">  pi = <span class="keyword">lambda</span> s: &#123; s: a <span class="keyword">for</span> s, a <span class="keyword">in</span> <span class="built_in">enumerate</span>(random_actions)&#125;[s]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># keep a copy of the policy</span></span><br><span class="line">    old_pi = &#123; s:pi(s) <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(P)) &#125;</span><br><span class="line">  </span><br><span class="line">    V = policy_evaluation(pi, P, gamma, theta)</span><br><span class="line">    pi = policy_improvement(V, P, gamma)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># check if the new policy is different</span></span><br><span class="line">    <span class="keyword">if</span> old_pi == &#123; s:pi(s) <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(P)):</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure>
<p>The <strong>policy iteration is guaranteed to converge to the exact
optimal policy</strong>: the mathematical proof shows it will not get
stuck in local optima.</p>
<h3 id="value-iteration-vi-improving-behaviors-early">Value iteration
(VI): Improving behaviors early</h3>
<p>VI can be thought of “greedily greedifying policies” because we
calculate the greedy policy as soon as we can, greedily. VI doesn’t wait
until we have an accurate estimate of the policy before it improves it,
but instead, VI truncates the policy-evaluation phase after a single
state sweep.</p>
<p>We can merge a truncated policy-evaluation step and a policy
improvement into the same equation.</p>
<p><span class="math display">\[
v_{k+1}(s) = \max_a \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma
v_k(s&#39;)]
\]</span></p>
<p>In VI, we don’t have to deal with policies at all. While the goal of
VI is the same as the goal of PI —— to find the optimal policy for a
given MDP —— VI happens to do this through the value functions.</p>
<p>Whereas VI and PI are two different algorithms, in a more general
view, they are two instances of <strong>generalized policy iteration
(GPI)</strong>. <em>GPI is a general idea in RL in which policies are
improved using their value function estimates, and value function
estimates are improved toward the actual value function for the current
policy.</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">value_iteration</span>(<span class="params">P, gamma=<span class="number">1.0</span>, theta=<span class="number">1e-10</span></span>):</span><br><span class="line">  V = np.zeros(<span class="built_in">len</span>(P), dtype=np.float64)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># Q has to be initialized to all zeros. Otherwise, the estimate would</span></span><br><span class="line">    <span class="comment"># be incorrect.</span></span><br><span class="line">    Q = np.zeros((<span class="built_in">len</span>(P), <span class="built_in">len</span>(P[<span class="number">0</span>])), dtype=np.float64)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(P)):</span><br><span class="line">      <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(P[s])):</span><br><span class="line">        <span class="keyword">for</span> prob, next_state, reward, done <span class="keyword">in</span> P[s][a]:</span><br><span class="line">          Q[s][a] += prob * (reward + gamma * V[next_status] * (<span class="keyword">not</span> done))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(V - np.<span class="built_in">max</span>(Q, axis=<span class="number">1</span>))) &lt; theta:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line">    V = np.<span class="built_in">max</span>(Q, axis=<span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">  pi = <span class="keyword">lambda</span> s: &#123;s:a <span class="keyword">for</span> s, a <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.argmax(Q, axis=<span class="number">1</span>))&#125;[s]</span><br><span class="line">  <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">

<script>
  function resizeIframe() {
    const obj = document.querySelector('.post-body iframe');
    obj.onload = () => {
      obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
      obj.setAttribute('scrolling', 'no');
    }
  }
  resizeIframe();
</script>
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>bifnudo
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://aquietzero.github.io/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/" title="Balancing immediate and long-term goals">https://aquietzero.github.io/2023/09/05/reading/grokking-deep-reinforcement-learning/2023-09-05-balancing-immediate-and-long-term-goals/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
              <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag"># 读书笔记</a>
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/05/reading/grokking-deep-reinforcement-learning/index/" rel="prev" title="Grokking Deep Reinforcement Learning">
                  <i class="fa fa-angle-left"></i> Grokking Deep Reinforcement Learning
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/09/13/reading/grokking-deep-reinforcement-learning/2023-09-13-balancing-the-gathering-and-use-of-information/" rel="next" title="Balancing the gathering and use of information">
                  Balancing the gathering and use of information <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">bifnudo</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">300k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">18:12</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/aquietzero" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"version":"11.5.0","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.0/mermaid.min.js","integrity":"sha256-K7oJiQlDulzl24ZUFOywuYme1JqBBvQzK6m8qHjt9Gk="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"aquietzero-github-io","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
