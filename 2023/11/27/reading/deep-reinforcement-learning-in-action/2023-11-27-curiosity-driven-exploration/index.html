<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CAgbalumo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"aquietzero.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"width":null},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="The games that DQN was successful at all gave relatively frequent rewards during game play and did not require significant long-term planning. But some other games, let’s say give a reward after the p">
<meta property="og:type" content="article">
<meta property="og:title" content="Curiosity-driven exploration">
<meta property="og:url" content="https://aquietzero.github.io/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/index.html">
<meta property="og:site_name" content="NullSpace">
<meta property="og:description" content="The games that DQN was successful at all gave relatively frequent rewards during game play and did not require significant long-term planning. But some other games, let’s say give a reward after the p">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-11-26T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-19T07:11:29.528Z">
<meta property="article:author" content="bifnudo">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://aquietzero.github.io/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://aquietzero.github.io/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/","path":"2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/","title":"Curiosity-driven exploration"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Curiosity-driven exploration | NullSpace</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BCZ3TL69CD"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-BCZ3TL69CD","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">NullSpace</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Stay hungry, stay foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">归档</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#tackling-sparse-rewards-with-predictive-coding"><span class="nav-number">1.</span> <span class="nav-text">Tackling sparse
rewards with predictive coding</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#inverse-dynamics-prediction"><span class="nav-number">2.</span> <span class="nav-text">Inverse dynamics prediction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#setting-up-super-mario-bros"><span class="nav-number">3.</span> <span class="nav-text">Setting up Super Mario Bros</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#preprocessing-and-the-q-network"><span class="nav-number">4.</span> <span class="nav-text">Preprocessing and the
Q-network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#setting-up-the-q-network-and-policy-function"><span class="nav-number">5.</span> <span class="nav-text">Setting up the
Q-network and policy function</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#policy-function"><span class="nav-number">5.1.</span> <span class="nav-text">Policy function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#experience-replay"><span class="nav-number">5.2.</span> <span class="nav-text">Experience replay</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#intrinsic-curiosity-module"><span class="nav-number">6.</span> <span class="nav-text">Intrinsic curiosity module</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#icm"><span class="nav-number">6.1.</span> <span class="nav-text">ICM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mini-batch-train"><span class="nav-number">6.2.</span> <span class="nav-text">Mini-batch train</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#main-training-loop"><span class="nav-number">6.3.</span> <span class="nav-text">Main training loop</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bifnudo"
      src="/assets/images/me.webp">
  <p class="site-author-name" itemprop="name">bifnudo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">130</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">66</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/aquietzero" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;aquietzero" rel="noopener me" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.douban.com/people/aquietzero" title="豆瓣 → https:&#x2F;&#x2F;www.douban.com&#x2F;people&#x2F;aquietzero" rel="noopener me" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhaoyunhaosss@gmail.com" title="E-Mail → mailto:zhaoyunhaosss@gmail.com" rel="noopener me" target="_blank">E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://aquietzero.github.io/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/images/me.webp">
      <meta itemprop="name" content="bifnudo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NullSpace">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Curiosity-driven exploration | NullSpace">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Curiosity-driven exploration
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-11-27 00:00:00" itemprop="dateCreated datePublished" datetime="2023-11-27T00:00:00+08:00">2023-11-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Reading/" itemprop="url" rel="index"><span itemprop="name">Reading</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>The games that DQN was successful at all gave relatively frequent
rewards during game play and did not require significant long-term
planning. But some other games, let’s say give a reward after the player
finds a key in the room. But it is extremely unlikely that the agent
will find the key and get a reward with this random exploration
policy.</p>
<p>This problem is called the <strong>sparse reward problem</strong>. If
the agent doesn’t observe enough reward signals to reinforce its
actions, it can’t learn.</p>
<h1 id="tackling-sparse-rewards-with-predictive-coding">Tackling sparse
rewards with predictive coding</h1>
<p>Curiosity can be thought of as a kind of desire to reduce the
uncertainty in your environment. One of the first attempts to imbue
reinforcement learning agents with a sense of curiosity involved using a
prediction error mechanism. <em>The idea was that in addition to trying
to maximize <strong>extrinsic rewards</strong>, the agent would also try
to predict the next state of the environment given its action, and it
would try to reduce its prediction error.</em></p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/the-idea.png" /></p>
<p>The idea is to sum the prediction error (which will be called
<strong>intrinsic reward</strong>) with the extrinsic reward and use
that total as the new reward signal for the environment.</p>
<aside>
<p>💡 If you train these agents in an environment that has a constant
source of randomness, such as TV screen playing random noise, the agent
will have a constantly high prediction error and will be unable to
reduce it.</p>
</aside>
<h1 id="inverse-dynamics-prediction">Inverse dynamics prediction</h1>
<p>The prediction error module is implemented as a function: <span
class="math inline">\(f: (S_t, a_t) \to \hat{S}_{t+1}\)</span>, that
takes a state and the action taken and returns the predicted next state.
It is predicting the future state of the environment, so we call it the
<strong>forward-prediction model</strong>.</p>
<p>We want to only predict aspects of the state that actually matter,
not parts that are trivial or noise. The way to build in the “doesn’t
matter” constraint to the prediction model is to add another model
called an <strong>inverse model</strong>, <span class="math inline">\(g:
(S_t, S_{t+1}) \to \hat{a}_t\)</span>. This is a function that takes a
state and the next state, and then returns a prediction for which action
was taken that led to the transition from <span
class="math inline">\(s_t\)</span> to <span
class="math inline">\(s_{t+1}\)</span>.</p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/prediction-and-inverse-module.png" /></p>
<p>There is another model that is tightly coupled to the inverse model
called the <strong>encoder model</strong>, denoted <span
class="math inline">\(\phi\)</span>. The encoder function, <span
class="math inline">\(\phi: S_t \to \tilde{S}_t\)</span>, takes a state
and returns an encoded state <span
class="math inline">\(\tilde{S}_t\)</span> such that the dimensionality
of <span class="math inline">\(\tilde{S}_t\)</span> is significantly
lower than the raw state <span class="math inline">\(S_t\)</span>.</p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/encoder.png" /></p>
<p>The encoder model is trained via the inverse model because we
actually use the encoded states as inputs to the forward and inverse
models <span class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span> rather than the raw states. That is,
the forward model becomes a function, <span class="math inline">\(f:
\phi(S_t) \times a_t \to \hat{\phi}(S_{t+1})\)</span>, where <span
class="math inline">\(\hat{\phi}(S_{t+1})\)</span> refers to a
prediction of the encoded state, and the inverse model becomes <span
class="math inline">\(g: \phi(S_t) \times \hat{\phi}(S_{t+1}) \to
\hat{a}_t\)</span>.</p>
<p>The encoder model isn’t trained directly —— it is not an
auto-encoder. <strong>It is only trained through the inverse
model.</strong></p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/curiosity-module.png" /></p>
<p>The curiosity module.</p>
<ol type="1">
<li>Encode states <span class="math inline">\(S_t\)</span> and <span
class="math inline">\(S_{t+1}\)</span> into low-dimensional vectors,
<span class="math inline">\(\phi(S_t)\)</span> and <span
class="math inline">\(\phi(S_{t+1})\)</span>.</li>
<li>The encoded states are passed to the forward and inverse
models.</li>
<li>The inverse model backpropagates to the encoded model.</li>
<li>The forward model is trained by backpropagating from its own error
function, but it does not backpropagate through to the encoder.</li>
</ol>
<h1 id="setting-up-super-mario-bros">Setting up Super Mario Bros</h1>
<p>The forward, inverse, and encoder models form the <strong>intrinsic
curiosity module (ICM)</strong>, which will be implemented below. The
ICM generates a new intrinsic reward signal based on information from
the environment, so it is independent of how the agent model is
implemented. To keep everything simple, we use a Q-learning model.</p>
<p>Install Super Mario Bros by <code>pip</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install gym-super-mario-bros</span><br></pre></td></tr></table></figure>
<p>After installing, it can be used as</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nes_py.wrappers <span class="keyword">import</span> JoypadSpace</span><br><span class="line"><span class="keyword">import</span> gym_super_mario_bros</span><br><span class="line"><span class="keyword">from</span> gym_super_mario_bros.actions <span class="keyword">import</span> SIMPLE_MOVEMENT, COMPLEX_MOVEMENT</span><br><span class="line"></span><br><span class="line">env = gym_super_mario_bros.make(</span><br><span class="line">    <span class="string">&#x27;SuperMarioBros-v0&#x27;</span>,</span><br><span class="line">    apply_api_compatibility=<span class="literal">True</span>,</span><br><span class="line">    render_mode=<span class="string">&#x27;human&#x27;</span>)</span><br><span class="line">env = JoypadSpace(env, COMPLEX_MOVEMENT)</span><br></pre></td></tr></table></figure>
<p><code>render_mode</code> specifies how the game is rendered. In most
of training processes below, the <code>render_mode</code> is
<code>rgb_array</code>, which returns an array of a frame in the game,
whose shape is <code>(x, y, 3)</code></p>
<p>Since <code>gym</code> now becomes <code>gymnasium</code>, all the
APIs have breaking changes, <code>apply_api_compatibility</code> should
be set to <code>True</code>.</p>
<h1 id="preprocessing-and-the-q-network">Preprocessing and the
Q-network</h1>
<p>The raw state is an RGB video frame with dimensions (240, 256, 3),
which is unnecessarily high-dimensional and would be computationally
costly for no advantage. We will convert these RGB states into grayscale
and resize them to 42x42 to allow our model to train much faster.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> skimage.transform <span class="keyword">import</span> resize</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">downscale_obs</span>(<span class="params">obs, new_size=(<span class="params"><span class="number">42</span>, <span class="number">42</span></span>), to_gray=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">if</span> to_gray:</span><br><span class="line">        <span class="keyword">return</span> resize(obs, new_size, anti_aliasing=<span class="literal">True</span>).<span class="built_in">max</span>(axis=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> resize(obs, new_size, anti_aliasing=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Then we will use the last three frames as a single state. This gives
our model access to velocity information.</p>
<figure>
<img
src="/assets/images/2023-11-27-curiosity-driven-exploration/frame-state.png"
alt="Each state given to the agent is a concatenation of the three most recent (grayscale) frames in the game. This is necessary so that the model can have access to not just the position of objects, but also their direction of movement." />
<figcaption aria-hidden="true">Each state given to the agent is a
concatenation of the three most recent (grayscale) frames in the game.
This is necessary so that the model can have access to not just the
position of objects, but also their direction of movement.</figcaption>
</figure>
<p>Each state given to the agent is a concatenation of the three most
recent (grayscale) frames in the game. This is necessary so that the
model can have access to not just the position of objects, but also
their direction of movement.</p>
<p>We need three functions to handle the states:</p>
<ul>
<li><code>prepare_state(state)</code>: Downscale state and converts to
grayscale, converts to a pytorch tensor and adds a batch dimension.</li>
<li><code>prepare_multi_state(state1, state2)</code>: Given an existing
3-frame state1 and a new single frame2, adds the latest frame to the
queue.</li>
<li><code>prepare_initial_state(state, N=3)</code>: Creates a state with
three copies of the same frame and adds a batch dimension.</li>
</ul>
<h1 id="setting-up-the-q-network-and-policy-function">Setting up the
Q-network and policy function</h1>
<p>DQN is used for our agent here. It is consisted with four layers of
convolution networks. The activation function used here is
<code>ELU</code>. DQN needs a policy function, which selects action from
the Q values return by DQN.</p>
<h2 id="policy-function">Policy function</h2>
<p>We will use a policy that begins with a softmax policy to encourage
exploration, and after a fixed number of game steps we will switch to an
epsilon-greedy strategy.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy</span>(<span class="params">qvalues, eps=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> eps <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> torch.rand(<span class="number">1</span>) &lt; eps:</span><br><span class="line">            <span class="keyword">return</span> torch.randint(low=<span class="number">0</span>, high=<span class="number">11</span>, size=(<span class="number">1</span>,))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> torch.argmax(qvalues)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> torch.multinomial(F.softmax(F.normalize(qvalues)),</span><br><span class="line">                               num_samples=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>If <code>eps</code> is not provided, we sample from the softmax of Q
values.</p>
<h2 id="experience-replay">Experience replay</h2>
<p>An experience replay class contains a list of experiences, each of
which is a tuple of <span class="math inline">\((S_t, a_t, r_t,
S_{t+1})\)</span>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExperienceReplay</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, N=<span class="number">500</span>, batch_size=<span class="number">100</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.N = N</span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="variable language_">self</span>.memory = []</span><br><span class="line">        <span class="variable language_">self</span>.counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_memory</span>(<span class="params">self, state1, action, reward, state2</span>):</span><br><span class="line">        <span class="variable language_">self</span>.counter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.counter % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="variable language_">self</span>.shuffle_memory()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if the memory is not full, adds to the list</span></span><br><span class="line">        <span class="comment"># otherwise replaces a random memory with the new one</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.memory) &lt; <span class="variable language_">self</span>.N:</span><br><span class="line">            <span class="variable language_">self</span>.memory.append((state1, action, reward, state2))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rand_index = np.random.randint(<span class="number">0</span>, <span class="variable language_">self</span>.N - <span class="number">1</span>)</span><br><span class="line">            <span class="variable language_">self</span>.memory[rand_index] = (state1, action, reward, state2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shuffle_memory</span>(<span class="params">self</span>):</span><br><span class="line">        shuffle(<span class="variable language_">self</span>.memory)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.memory) &lt; <span class="variable language_">self</span>.batch_size:</span><br><span class="line">            batch_size = <span class="built_in">len</span>(<span class="variable language_">self</span>.memory)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch_size = <span class="variable language_">self</span>.batch_size</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.memory) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Error: No data in memory.&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        indexes = np.random.choice(</span><br><span class="line">                  np.arange(<span class="built_in">len</span>(<span class="variable language_">self</span>.memory)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">        batch = [<span class="variable language_">self</span>.memory[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes]</span><br><span class="line">        state1_batch = torch.stack([x[<span class="number">0</span>].squeeze(dim=<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> batch], dim=<span class="number">0</span>)</span><br><span class="line">        action_batch = torch.Tensor([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> batch]).long()</span><br><span class="line">        reward_batch = torch.Tensor([x[<span class="number">2</span>] <span class="keyword">for</span> x <span class="keyword">in</span> batch])</span><br><span class="line">        state2_batch = torch.stack([x[<span class="number">3</span>].squeeze(dim=<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> batch], dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> state1_batch, action_batch, reward_batch, state2_batch</span><br></pre></td></tr></table></figure>
<h1 id="intrinsic-curiosity-module">Intrinsic curiosity module</h1>
<p>A high-level overview of the intrinsic curiosity module (ICM). The
ICM has three components that are each separate neural networks. The
encoder model encodes states into a low-dimensional vector, and it is
trained indirectly through the inverse model, which tries to predict the
action that was taken given two consecutive states. The forward model
predicts the next encoded state, and its error is the prediction error
that is used as the intrinsic reward.</p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/intrinsic-reward.png" /></p>
<p>Below shows the type and dimensionality of the inputs and outputs of
each component of the ICM.</p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/icm.png" /></p>
<p>The DQN and the ICM contribute to a single overall loss function that
is given to the optimizer to minimize with respect to the DQN and ICM
parameters. The DQN’s Q value predictions are compared to the observed
rewards. The observed rewards, however, are summed together with the
ICM’s prediction error to get a new reward value.</p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/icm-prediction-error.png" /></p>
<p>A complete view of the overall algorithm, including the ICM. First we
generate <span class="math inline">\(B\)</span> samples from the
experience replay memory and use these for the ICM and DQN. We run the
ICM forward to generate a prediction error, which is then provided to
the DQN’s error function. The DQN learns to predict action values that
reflect not only extrinsic (environment provided) rewards but also an
intrinsic (prediction error-based) reward.</p>
<p><img
src="/assets/images/2023-11-27-curiosity-driven-exploration/icm-detail.png" /></p>
<ul>
<li>The forward model is a simple two-layer neural network with linear
layers.</li>
<li>The inverse model is also a simple two-layer neural network with
linear layers.</li>
<li>The encoder is a neural network composed of four convolutional
layers (with an identical architecture to the DQN)</li>
</ul>
<p>The overall loss function defined by all four models is</p>
<p><span class="math display">\[
\text{minimize} [\lambda \cdot Q_\text{loss} + (1 - \beta)F_\text{loss}
+ \beta \cdot G_\text{loss}]
\]</span></p>
<h2 id="icm">ICM</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ICM</span>(<span class="params">state1, action, state2, forward_scale=<span class="number">1.</span>, inverse_scale=<span class="number">1e4</span></span>):</span><br><span class="line">    <span class="comment"># encodes state1 and state2</span></span><br><span class="line">    state1_hat = encoder(state1)</span><br><span class="line">    state2_hat = encoder(state2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># runs forward model to get state2 prediction</span></span><br><span class="line">    state2_hat_pred = forward_model(state1_hat.detach(), action.detach())</span><br><span class="line">    <span class="comment"># calculates forward loss</span></span><br><span class="line">    forward_pred_err = forward_scale * \</span><br><span class="line">        forward_loss(state2_hat_pred, state2_hat.detach()).<span class="built_in">sum</span>(dim=<span class="number">1</span>).unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># runs inverse model to get action prediction</span></span><br><span class="line">    pred_action = inverse_model(state1_hat, state2_hat)</span><br><span class="line">    <span class="comment"># calculates inverse loss</span></span><br><span class="line">    inverse_pred_err = inverse_scale * \</span><br><span class="line">        inverse_loss(pred_action, action.detach().flatten()).unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> forward_pred_err, inverse_pred_err</span><br></pre></td></tr></table></figure>
<h2 id="mini-batch-train">Mini-batch train</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minibatch_train</span>(<span class="params">use_extrinsic=<span class="literal">True</span></span>):</span><br><span class="line">    state1_batch, action_batch, reward_batch, state2_batch = replay.get_batch()</span><br><span class="line">    action_batch = action_batch.view(action_batch.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">    reward_batch = reward_batch.view(reward_batch.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># gets losses of forward model and inverse model</span></span><br><span class="line">    forward_pred_err, inverse_pred_err = ICM(state1_batch, action_batch, state2_batch)</span><br><span class="line">    i_reward = (<span class="number">1.</span> / params[<span class="string">&#x27;eta&#x27;</span>]) * forward_pred_err</span><br><span class="line">    reward = i_reward.detach()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_extrinsic:</span><br><span class="line">        reward += reward_batch</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculates Q values for the next state</span></span><br><span class="line">    qvals = Qmodel(state2_batch)</span><br><span class="line">    reward += params[<span class="string">&#x27;gamma&#x27;</span>] * torch.<span class="built_in">max</span>(qvals)</span><br><span class="line">    reward_pred = Qmodel(state1_batch)</span><br><span class="line">    reward_target = reward_pred.clone()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># since the action_batch is a tensor of integers of action indices,</span></span><br><span class="line">    <span class="comment"># we convert this to a tensor of one-hot encoded vectors.</span></span><br><span class="line">    indices = torch.stack((torch.arange(action_batch.shape[<span class="number">0</span>]), action_batch.squeeze()), dim=<span class="number">0</span>)</span><br><span class="line">    indices = indices.tolist()</span><br><span class="line">    reward_target[indices] = reward.squeeze()</span><br><span class="line">    q_loss = <span class="number">1e5</span> * qloss(F.normalize(reward_pred), F.normalize(reward_target.detach()))</span><br><span class="line">    <span class="keyword">return</span> forward_pred_err, inverse_pred_err, q_loss</span><br></pre></td></tr></table></figure>
<h2 id="main-training-loop">Main training loop</h2>
<p>The final training process is defined as follows:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</span><br><span class="line"></span><br><span class="line">env = get_env(<span class="string">&#x27;rgb_array&#x27;</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">3500</span></span><br><span class="line">env.reset()</span><br><span class="line"></span><br><span class="line">state1 = prepare_initial_state(env.render())</span><br><span class="line">eps = <span class="number">0.15</span> <span class="comment"># epsilon in policy method</span></span><br><span class="line">losses = []</span><br><span class="line">episode_length = <span class="number">0</span></span><br><span class="line">switch_to_eps_greedy = <span class="number">1000</span> <span class="comment"># use epsilon greedy after 1000 episodes</span></span><br><span class="line">state_deque = deque(maxlen=params[<span class="string">&#x27;frames_per_state&#x27;</span>])</span><br><span class="line">e_reward = <span class="number">0.</span></span><br><span class="line"><span class="comment"># last_x_pos = env.env.env._x_position</span></span><br><span class="line">last_x_pos = <span class="number">0</span></span><br><span class="line">ep_lengths = []</span><br><span class="line">use_extrinsic = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    opt.zero_grad()</span><br><span class="line">    episode_length += <span class="number">1</span></span><br><span class="line">    <span class="comment"># runs DQN to get Q values</span></span><br><span class="line">    q_val_pred = Qmodel(state1)</span><br><span class="line">    <span class="keyword">if</span> i &gt; switch_to_eps_greedy:</span><br><span class="line">        action = <span class="built_in">int</span>(policy(q_val_pred, eps))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        action = <span class="built_in">int</span>(policy(q_val_pred))</span><br><span class="line">      </span><br><span class="line">    <span class="comment"># repeats actions for accelerating learning process</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(params[<span class="string">&#x27;action_repeats&#x27;</span>]):</span><br><span class="line">        state2, e_reward_, done, truncated, info = env.step(action)</span><br><span class="line">        last_x_pos = info[<span class="string">&#x27;x_pos&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            state1 = reset_env()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        e_reward += e_reward_</span><br><span class="line">        state_deque.append(prepare_state(state2))</span><br><span class="line">  </span><br><span class="line">    state2 = torch.stack(<span class="built_in">list</span>(state_deque), dim=<span class="number">1</span>)</span><br><span class="line">    replay.add_memory(state1, action, e_reward, state2)</span><br><span class="line">    e_reward = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> episode_length &gt; params[<span class="string">&#x27;max_episode_len&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> (info[<span class="string">&#x27;x_pos&#x27;</span>] - last_x_pos) &lt; params[<span class="string">&#x27;min_progress&#x27;</span>]:</span><br><span class="line">            done = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            last_x_pos = info[<span class="string">&#x27;x_pos&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        ep_lengths.append(info[<span class="string">&#x27;x_pos&#x27;</span>])</span><br><span class="line">        state1 = reset_env()</span><br><span class="line">        <span class="comment"># last_x_pos = env.env.env._x_position</span></span><br><span class="line">        last_x_pos = info[<span class="string">&#x27;x_pos&#x27;</span>]</span><br><span class="line">        episode_length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        state1 = state2</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(replay.memory) &lt; params[<span class="string">&#x27;batch_size&#x27;</span>]:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">  </span><br><span class="line">    forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_extrinsic=<span class="literal">False</span>)</span><br><span class="line">    loss = loss_fun(q_loss, forward_pred_err, inverse_pred_err)</span><br><span class="line">    loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean())</span><br><span class="line">    losses.append(loss_list)</span><br><span class="line">    loss.backward()</span><br><span class="line">    opt.step()</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">

<script>
  function resizeIframe() {
    const obj = document.querySelector('.post-body iframe');
    obj.onload = () => {
      obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
      obj.setAttribute('scrolling', 'no');
    }
  }
  resizeIframe();
</script>
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>bifnudo
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://aquietzero.github.io/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/" title="Curiosity-driven exploration">https://aquietzero.github.io/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-curiosity-driven-exploration/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/11/27/reading/deep-reinforcement-learning-in-action/2023-11-27-alternative-optimization-methods-evolutionary-algorithms/" rel="prev" title="Alternative optimization methods - Evolutionary algorithms">
                  <i class="fa fa-angle-left"></i> Alternative optimization methods - Evolutionary algorithms
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/07/03/life/2024-07-03-%E5%BE%80%E4%BA%8B%E7%B3%BB%E5%88%97/" rel="next" title="往事系列">
                  往事系列 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">bifnudo</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">319k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">19:21</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/aquietzero" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"version":"11.5.0","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.0/mermaid.min.js","integrity":"sha256-K7oJiQlDulzl24ZUFOywuYme1JqBBvQzK6m8qHjt9Gk="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"aquietzero-github-io","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
